# Real-Time Stock Tracking Pipeline âœ¨

A **production-ready, real-time data pipeline** that ingests stock market data from Finnhub API, processes it through Apache Kafka and Airflow, transforms it using DBT, stores it in Snowflake, and visualizes it in Google Looker Studio.

**Status:** âœ… **90% Complete** | **Production Ready** ğŸš€

---

## ğŸ“Š Quick Demo

```
Finnhub API (Every 60s)
    â†“
Stock Producer (Docker)
    â†“
Kafka Topic (stock_prices)
    â†“
Airflow DAG (Every 5 min)
    â†“
Snowflake Database
    â”œâ”€â”€ RAW_STOCKS.STOCK_PRICES (Raw data)
    â”œâ”€â”€ STG_STOCKS.STG_STOCK_PRICES (Cleaned)
    â”œâ”€â”€ INT_STOCKS.INT_STOCK_DAILY_AGGREGATES (Aggregated)
    â””â”€â”€ MARTS_STOCKS.STOCK_ANALYTICS (Ready for dashboards)
    â†“
Google Looker Studio (Beautiful dashboards!)
```

---

## ğŸ¯ Key Features

âœ… **Real-time Data Ingestion**
- Fetches 10 stocks every 60 seconds from Finnhub API
- Rate limiting & exponential backoff retry logic
- Publishes to Apache Kafka for streaming

âœ… **Orchestration & Scheduling**
- Apache Airflow DAGs running every 5 minutes
- Automated Kafka consumption
- Snowflake data insertion with error handling

âœ… **Cloud Data Warehouse**
- Snowflake for scalable storage
- 4 schema layers (raw, staging, intermediate, marts)
- 1000+ daily records ingested

âœ… **Data Transformation (ELT)**
- DBT with 3-layer architecture
- Data quality checks & validation
- Technical indicators calculation
- Daily OHLC aggregation

âœ… **Analytics Dashboards**
- Google Looker Studio (FREE)
- Interactive visualizations
- Auto-refresh every 5 minutes
- Stock price trends & performance metrics

âœ… **Monitoring & Observability**
- Prometheus for metrics collection
- Grafana dashboards
- Application & infrastructure logs

âœ… **Fully Containerized**
- Docker Compose with 11 services
- Single command deployment
- Development & production ready

---

## ğŸ“ˆ Current Progress

| Phase | Component | Status | Completion |
|-------|-----------|--------|-----------|
| Infrastructure | Docker & Services | âœ… Complete | 100% |
| Ingestion | Finnhub API + Kafka | âœ… Complete | 100% |
| Orchestration | Airflow DAGs | âœ… Complete | 100% |
| Warehouse | Snowflake Setup | âœ… Complete | 100% |
| Transformation | DBT 3-Layer | âœ… Complete | 100% |
| Security | Credentials Management | âœ… Complete | 100% |
| Monitoring | Prometheus & Grafana | â³ Partial | 50% |
| Visualization | Looker Studio | âœ… Complete | 100% |

**Overall: 90% Complete** ğŸ‰

---

## ğŸ—ï¸ Architecture

See [ARCHITECTURE.md](./ARCHITECTURE.md) for detailed system design.

### High-Level Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Finnhub API â”‚â”€â”€â”€â”€â–¶â”‚   Stock  â”‚â”€â”€â”€â”€â–¶â”‚  Kafka   â”‚â”€â”€â”€â”€â–¶â”‚   Airflow    â”‚
â”‚ (60 sec)    â”‚     â”‚ Producer â”‚     â”‚  Topic   â”‚     â”‚  (5 min DAG) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                              â”‚
                                                              â–¼
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚   Snowflake DB   â”‚
                                                    â”‚  STOCKS_FIN      â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                      â”‚    â”‚    â”‚    â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”¬â”€â”€â”€â”´â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”¤
                                    â–¼                 â–¼     â–¼       â–¼
                            RAW_STOCKS        STG_STOCKS INT_STOCKS MARTS_STOCKS
                            (Raw Data)        (Cleaned)  (Aggregated) (Analytics)
                                                              â”‚
                                                              â–¼
                                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                    â”‚  Looker Studio   â”‚
                                                    â”‚  Dashboards      â”‚
                                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start (5 Minutes)

### Prerequisites

- **Docker Desktop** (https://www.docker.com/products/docker-desktop)
- **Python 3.11+** (https://www.python.org/)
- **Snowflake Account** (free trial: https://signup.snowflake.com/)
- **Finnhub API Key** (free: https://finnhub.io/)

### Setup

```bash
# 1. Clone repository
git clone https://github.com/YOUR_USERNAME/stock-tracking-pipeline.git
cd stock-tracking-pipeline

# 2. Create credentials file
cp secrets.env.example secrets.env

# 3. Edit secrets.env with YOUR credentials
# Windows: notepad secrets.env
# Mac/Linux: nano secrets.env

# 4. Start all services
docker-compose up -d

# 5. Verify services (should show 11 containers)
docker-compose ps
```

**Done!** Your pipeline is running! ğŸ‰

### Access Services

- **Airflow UI:** http://localhost:8888 (admin/admin)
- **Kafka UI:** http://localhost:8080
- **Grafana:** http://localhost:3000 (admin/admin)
- **Prometheus:** http://localhost:9090

---

## ğŸ“š Documentation

- **[SETUP.md](./SETUP.md)** - Detailed setup instructions
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System architecture & design
- **[README.md](./README.md)** - This file

---

## ğŸ“ Project Structure

```
stock-tracking-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yml              â† All services config
â”œâ”€â”€ load_env.ps1                    â† Load credentials (PowerShell)
â”œâ”€â”€ .gitignore                      â† Git ignore rules
â”‚
â”œâ”€â”€ secrets.env.example             â† Template for credentials
â”œâ”€â”€ profiles.yml.example            â† Template for DBT
â”‚
â”œâ”€â”€ README.md                       â† This file
â”œâ”€â”€ SETUP.md                        â† Setup instructions
â”œâ”€â”€ ARCHITECTURE.md                 â† System design
â”‚
â”œâ”€â”€ airflow/                        â† Apache Airflow
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ stock_ingestion_dag.py  â† Main DAG
â”‚   â”œâ”€â”€ plugins/
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data-ingestion/                 â† Stock Producer
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ stock_producer.py       â† Finnhub fetcher
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ dbt/                            â† Data transformation
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ profiles.yml.example
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â””â”€â”€ stg_stock_prices.sql
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”‚   â””â”€â”€ int_stock_daily_aggregates.sql
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â””â”€â”€ marts_stock_analytics.sql
â”‚   â””â”€â”€ macros/
â”‚       â””â”€â”€ generate_schema_name.sql
â”‚
â”œâ”€â”€ infrastructure/                 â† Supporting services
â”‚   â”œâ”€â”€ postgres/
â”‚   â”œâ”€â”€ localstack/
â”‚   â””â”€â”€ monitoring/
â”‚
â”œâ”€â”€ monitoring/                     â† Grafana & Prometheus configs
â”œâ”€â”€ logs/                          â† Application logs
â”œâ”€â”€ docs/                          â† Additional documentation
â””â”€â”€ great-expectations/            â† Data quality (optional)
```

---

## ğŸ”§ Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **API** | Finnhub | Stock data source |
| **Streaming** | Apache Kafka | Message queue |
| **Orchestration** | Apache Airflow | DAG scheduling |
| **Warehouse** | Snowflake | Cloud data storage |
| **Transform** | DBT 1.7.0 | ELT transformations |
| **Visualization** | Looker Studio | Dashboards & reports |
| **Monitoring** | Prometheus & Grafana | System health |
| **Container** | Docker Compose | Infrastructure |
| **Language** | Python 3.11 | Code |

---

## ğŸ“Š Stocks Tracked

By default, the pipeline tracks **10 stocks**:

```
AAPL (Apple)      GOOGL (Google)    MSFT (Microsoft)   AMZN (Amazon)
TSLA (Tesla)      NVDA (NVIDIA)     META (Meta)        NFLX (Netflix)
UBER (Uber)       COIN (Coinbase)
```

**Customize** by editing `STOCKS` in `secrets.env`:
```env
STOCKS=AAPL,GOOGL,MSFT  # Your custom list
```

---

## ğŸ“ˆ Data Pipeline

### 1. Ingestion (Every 60 seconds)
- Stock Producer fetches prices from Finnhub API
- 2-second rate limiting between requests
- Publishes JSON to Kafka topic `stock_prices`
- Auto-retry with exponential backoff

### 2. Orchestration (Every 5 minutes)
- Airflow DAG: `stock_data_ingestion_snowflake`
- **Task 1:** Consume messages from Kafka
- **Task 2:** Insert into Snowflake RAW_STOCKS.STOCK_PRICES
- **Task 3:** Log statistics

### 3. Transformation (Via DBT)
- **Staging:** Clean & validate data
  - Remove invalid records (price <= 0, volume < 0)
  - Detect price spikes (>20% change)
  - Extract date/hour, calculate ingest lag
  
- **Intermediate:** Daily aggregation
  - Calculate OHLC (Open, High, Low, Close)
  - Volume statistics (total, average)
  - Daily price change percentage
  - Deduplicate records
  
- **Marts:** Analytics-ready
  - 7-day moving average
  - 30-day moving average
  - 30-day volatility
  - 52-week highs/lows
  - Technical indicators

### 4. Visualization (Looker Studio)
- Connect to MARTS_STOCKS.STOCK_ANALYTICS table
- Interactive dashboards with filters
- Auto-refresh every 5 minutes

---

## ğŸ” Security

### Credentials Management

âœ… **Safe:**
- Credentials stored in `secrets.env` (local only)
- `secrets.env` is in `.gitignore` (never committed)
- Code uses environment variables (no hardcoding)
- `secrets.env.example` template provided (no real credentials)

âŒ **Never:**
- Commit `secrets.env` to GitHub
- Hardcode passwords in code
- Share credentials publicly
- Use same password for all environments

### How It Works

```
secrets.env (local, git-ignored)
    â†“
Environment variables
    â†“
docker-compose.yml references ${VAR_NAME}
    â†“
Containers use credentials securely
```

---

## ğŸš€ Deployment Steps

### Step 1: Prepare
```bash
cp secrets.env.example secrets.env
# Edit with YOUR credentials
```

### Step 2: Create Snowflake Database
```sql
CREATE DATABASE STOCKS_FIN;
CREATE SCHEMA STOCKS_FIN.RAW_STOCKS;
CREATE SCHEMA STOCKS_FIN.STG_STOCKS;
CREATE SCHEMA STOCKS_FIN.INT_STOCKS;
CREATE SCHEMA STOCKS_FIN.MARTS_STOCKS;
```

### Step 3: Start Services
```bash
docker-compose up -d
```

### Step 4: Set Up DBT (Windows)
```powershell
python -m venv dbt_env
.\dbt_env\Scripts\Activate.ps1
pip install dbt-core==1.7.0 dbt-snowflake==1.7.0
cp dbt/profiles.yml.example ~/.dbt/profiles.yml
cd dbt
dbt run
```

### Step 5: Create Looker Studio Dashboard
1. Go to https://lookerstudio.google.com
2. Create report â†’ Add Snowflake data source
3. Connect to STOCKS_FIN.MARTS_STOCKS.STOCK_ANALYTICS
4. Build visualizations

**See [SETUP.md](./SETUP.md) for detailed instructions.**

---

## ğŸ§ª Verification

### Check Services Running
```bash
docker-compose ps
# Should show 11 containers all "Up"
```

### Verify Data Flow
```bash
# In Snowflake:
SELECT COUNT(*) FROM STOCKS_FIN.RAW_STOCKS.STOCK_PRICES;
SELECT COUNT(*) FROM STOCKS_FIN.MARTS_STOCKS.STOCK_ANALYTICS;
# Should see 1000+ records in raw, 100+ in analytics
```

### Check Airflow DAG
1. Open http://localhost:8888
2. Look for: `stock_data_ingestion_snowflake`
3. Should show successful runs every 5 minutes

---

## ğŸ“Š Dashboards

### Looker Studio
**Visualizations:**
- Stock Price Trends (Time Series)
- Performance Table (Key Metrics)
- Interactive Filters (Symbol, Date Range)
- Auto-refresh (Every 5 minutes)

### Grafana
**Monitoring:**
- System health
- Container metrics
- Airflow DAG status
- Snowflake query performance

---

## ğŸ”§ Troubleshooting

### Containers Not Starting
```bash
docker-compose logs -f
# Check specific container:
docker-compose logs airflow-webserver
```

### Snowflake Connection Failed
1. Verify account ID in `secrets.env` (format: XXXXXX-XXXXXX)
2. Check username and password are correct
3. Ensure warehouse COMPUTE_WH is running

### Airflow DAG Not Running
1. Enable DAG in Airflow UI (toggle switch)
2. Verify Kafka is running: `docker-compose logs kafka`
3. Check Snowflake connection in DAG logs

### DBT Connection Error
```bash
powershell -ExecutionPolicy Bypass -File .\load_env.ps1
.\dbt_env\Scripts\Activate.ps1
cd dbt
dbt debug
```

**Full troubleshooting guide:** See [SETUP.md](./SETUP.md)

---

## ğŸ“š Additional Resources

- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - Detailed system design
- **[SETUP.md](./SETUP.md)** - Complete setup guide
- **Finnhub API:** https://finnhub.io/docs/api
- **DBT Docs:** https://docs.getdbt.com/
- **Snowflake Docs:** https://docs.snowflake.com/
- **Airflow Docs:** https://airflow.apache.org/docs/

---

## ğŸ¯ Next Steps

- [ ] Set up monitoring dashboards in Grafana
- [ ] Create data quality tests with Great Expectations
- [ ] Add email alerts for pipeline failures
- [ ] Implement Snowflake auto-clustering
- [ ] Add more technical indicators to DBT
- [ ] Deploy to production (AWS, GCP, Azure)

---

## ğŸ“ Support

### Troubleshooting
1. Check [SETUP.md](./SETUP.md) Troubleshooting section
2. Review container logs: `docker-compose logs`
3. Check [ARCHITECTURE.md](./ARCHITECTURE.md) for system design

### Questions
- Open an Issue on GitHub
- Check existing documentation
- Review code comments

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](./LICENSE) file for details.

---

## â­ Show Your Support

If you found this project helpful, please give it a star! â­

---

## ğŸ‘¤ Author

Real-time stock tracking pipeline built with modern data stack.

**Stack:** Finnhub â†’ Kafka â†’ Airflow â†’ Snowflake â†’ DBT â†’ Looker Studio

---

**Last Updated:** January 2025
**Version:** 1.0.0
**Status:** âœ… Production Ready

---

## ğŸ—ºï¸ Roadmap

### Phase 1: Core Pipeline âœ… DONE
- [x] Finnhub API integration
- [x] Kafka streaming
- [x] Airflow orchestration
- [x] Snowflake warehouse
- [x] DBT transformations
- [x] Looker Studio dashboards

### Phase 2: Monitoring â³ IN PROGRESS
- [ ] Grafana dashboards
- [ ] Alert rules
- [ ] Data quality checks



