services:
  # ==================== KAFKA SERVICES ====================
  # Using Kafka 7.4.0 (LTS, production-tested) instead of 7.5.0
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - stock-pipeline
    healthcheck:
      test: ["CMD", "echo", "ruok", "|", "nc", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    networks:
      - stock-pipeline
    volumes:
      - kafka_data:/var/lib/kafka/data

  kafka-ui:
    image: provectuslabs/kafka-ui:v0.7.2
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - stock-pipeline

  # ==================== STORAGE SERVICES ====================
  # LocalStack 3.0.0 (Stable, production-tested)
  localstack:
    image: localstack/localstack:3.0.0
    container_name: localstack
    ports:
      - "4566:4566"  # LocalStack gateway
      - "4571:4571"  # S3 service
    environment:
      SERVICES: s3
      DEBUG: 1
      DATA_DIR: /tmp/localstack/data
      LAMBDA_EXECUTOR: docker
      DOCKER_HOST: unix:///var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
    volumes:
      - "./infrastructure/localstack/init_localstack.sh:/docker-entrypoint-initaws.d/init_localstack.sh"
      - localstack_data:/tmp/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - stock-pipeline
    healthcheck:
      test: ["CMD", "awslocal", "s3", "ls"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PostgreSQL 14-alpine (LTS, stable, widely used)
  postgres:
    image: postgres:14-alpine
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./infrastructure/postgres/init_postgres.sql:/docker-entrypoint-initdb.d/init_postgres.sql
    networks:
      - stock-pipeline
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==================== AIRFLOW SERVICES ====================
  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
    env_file:
      - ./secrets.env
    environment:
      AIRFLOW_HOME: /home/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /home/airflow/dags
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: "False"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW_CONN_KAFKA: kafka://kafka:29092
      AIRFLOW_CONN_LOCALSTACK: s3://test:test@localhost/?host_name=localstack&port=4566&verify=False
      AIRFLOW_CONN_POSTGRES: postgresql://airflow:airflow@postgres:5432/airflow
      PYTHONUNBUFFERED: 1
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
      AWS_DEFAULT_REGION: us-east-1
    ports:
      - "8888:8080"
    volumes:
      - ./airflow/dags:/home/airflow/dags
      - ./airflow/plugins:/home/airflow/plugins
      - ./airflow/logs:/home/airflow/logs
      - ./airflow/config:/home/airflow/config
    networks:
      - stock-pipeline
    command: >
      bash -c "airflow db migrate &&
               airflow users create --role Admin --username admin --email admin@example.com --firstname Admin --lastname User --password admin &&
               airflow webserver"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 15s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_healthy
    env_file:
      - ./secrets.env
    environment:
      AIRFLOW_HOME: /home/airflow
      AIRFLOW__CORE__DAGS_FOLDER: /home/airflow/dags
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW_CONN_KAFKA: kafka://kafka:29092
      AIRFLOW_CONN_LOCALSTACK: s3://test:test@localhost/?host_name=localstack&port=4566&verify=False
      PYTHONUNBUFFERED: 1
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
      AWS_DEFAULT_REGION: us-east-1
    volumes:
      - ./airflow/dags:/home/airflow/dags
      - ./airflow/plugins:/home/airflow/plugins
      - ./airflow/logs:/home/airflow/logs
      - ./airflow/config:/home/airflow/config
    networks:
      - stock-pipeline
    command: airflow scheduler
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8793/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ==================== MONITORING SERVICES ====================
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - stock-pipeline
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ==================== DATA INGESTION SERVICES ====================
  stock-producer:
    build:
      context: ./data-ingestion
      dockerfile: Dockerfile
    container_name: stock-producer
    env_file:
      - ./secrets.env
    depends_on:
      - kafka
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC: stock_prices
      LOG_LEVEL: INFO
      PYTHONUNBUFFERED: 1
      STOCKS: "AAPL,GOOGL,MSFT,AMZN,TSLA,NVDA,META,NFLX,UBER,COIN"  # Top 10 stocks
      FETCH_INTERVAL: 60  # 60 seconds for live data
    volumes:
      - ./data-ingestion/src:/app/src
      - ./data-ingestion/logs:/app/logs
    networks:
      - stock-pipeline
    healthcheck:
      test: ["CMD", "pgrep", "-f", "python", "-a"]
      interval: 15s
      timeout: 10s
      retries: 5
    restart: unless-stopped

networks:
  stock-pipeline:
    driver: bridge

volumes:
  kafka_data:
    driver: local
  localstack_data:
    driver: local
  postgres_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
